{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f20d516",
   "metadata": {
    "tags": []
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/dask/dask/main/docs/source/images/dask_horizontal.svg\"\n",
    "     width=\"60%\"\n",
    "     alt=\"Dask logo\\\" />\n",
    "\n",
    "# Process Tabular Data with Dask DataFrame\n",
    "In this notebook we will learn about the [Dask DataFrame](https://docs.dask.org/en/latest/dataframe.html), a tabular DataFrame interface based on pandas that will automatically build parallel computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e457f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## When to use Dask DataFrames\n",
    "\n",
    "Pandas is great for tabular datasets that fit in memory. If your data fits in memory then you should use Pandas. **Dask becomes useful when the dataset you want to analyze is larger than your machine's RAM** where you would normally run into `MemoryError`s.\n",
    "\n",
    "```python\n",
    "    MemoryError:  ...\n",
    "```\n",
    "\n",
    "This also means:\n",
    "\n",
    "## Don't use Dask DataFrames if you don't need to!\n",
    "Distributed computing brings a lot of additional complexity into the mix and will **incur overhead**. If your dataset and computations fit comfortably within your local resources **this overhead will may be larger than the performance gain** you'll get by using Dask. In that case, stick with non-distributed libraries like pandas, numpy and scikit-learn. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389b5226",
   "metadata": {
    "tags": []
   },
   "source": [
    "## About this notebook\n",
    "During this tutorial, we will work with a dataset containg NYC flight data. This dataset is only about 200MB on disk so that you can download it in a reasonable time and exercises finish quickly, but Dask Dataframes will scale to datasets much larger than the memory on your local machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd61890f",
   "metadata": {},
   "source": [
    "## Getting started with Dask DataFrames\n",
    "\n",
    "Let's use Dask DataFrame's to explore the NYC flight dataset. Dask's `read_csv` function supports wildcard characters like `\"*\"` which can be used to load an entire directory of CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c69dd555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_dir='c:\\\\Users\\\\troy5\\\\OneDrive\\\\Desktop\\\\dask-elearning\\\\data'\n"
     ]
    }
   ],
   "source": [
    "%run prep_data.py -d flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5c3b20b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.path.join('data', 'nycflights', '*.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11b23870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a7df915",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:640: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n",
      "c:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:195: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  df = reader(bio, **kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+-------------------+---------+----------+\n| Column            | Found   | Expected |\n+-------------------+---------+----------+\n| ActualElapsedTime | float64 | int64    |\n| ArrDelay          | float64 | int64    |\n| ArrTime           | float64 | int64    |\n| DepDelay          | float64 | int64    |\n| DepTime           | float64 | int64    |\n| Distance          | float64 | int64    |\n+-------------------+---------+----------+\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'ActualElapsedTime': 'float64',\n       'ArrDelay': 'float64',\n       'ArrTime': 'float64',\n       'DepDelay': 'float64',\n       'DepTime': 'float64',\n       'Distance': 'float64'}\n\nto the call to `read_csv`/`read_table`.\n\nAlternatively, provide `assume_missing=True` to interpret\nall unspecified integer columns as floats.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m ddf \u001b[38;5;241m=\u001b[39m dd\u001b[38;5;241m.\u001b[39mread_csv(files,\n\u001b[0;32m      2\u001b[0m                  parse_dates\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m]},\n\u001b[0;32m      3\u001b[0m                  dtype\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTailNum\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m      4\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRSElapsedTime\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[0;32m      5\u001b[0m                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCancelled\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mbool\u001b[39m})\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28mprint\u001b[39m(ddf\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_collection.py:702\u001b[0m, in \u001b[0;36mFrameBase.head\u001b[1;34m(self, n, npartitions, compute)\u001b[0m\n\u001b[0;32m    700\u001b[0m out \u001b[38;5;241m=\u001b[39m new_collection(expr\u001b[38;5;241m.\u001b[39mHead(\u001b[38;5;28mself\u001b[39m, n\u001b[38;5;241m=\u001b[39mn, npartitions\u001b[38;5;241m=\u001b[39mnpartitions))\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute:\n\u001b[1;32m--> 702\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mcompute()\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_collection.py:476\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    474\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    475\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[1;32m--> 476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\base.py:375\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    352\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[0;32m    353\u001b[0m \n\u001b[0;32m    354\u001b[0m \u001b[38;5;124;03m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m    dask.compute\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 375\u001b[0m     (result,) \u001b[38;5;241m=\u001b[39m compute(\u001b[38;5;28mself\u001b[39m, traverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[1;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[0;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[1;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m schedule(dsk, keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_expr.py:3723\u001b[0m, in \u001b[0;36mFused._execute_task\u001b[1;34m(graph, name, *deps)\u001b[0m\n\u001b[0;32m   3721\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, dep \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(deps):\n\u001b[0;32m   3722\u001b[0m     graph[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i)] \u001b[38;5;241m=\u001b[39m dep\n\u001b[1;32m-> 3723\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dask\u001b[38;5;241m.\u001b[39mcore\u001b[38;5;241m.\u001b[39mget(graph, name)\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:142\u001b[0m, in \u001b[0;36mCSVFunctionWrapper.__call__\u001b[1;34m(self, part)\u001b[0m\n\u001b[0;32m    139\u001b[0m         rest_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musecols\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m columns\n\u001b[0;32m    141\u001b[0m \u001b[38;5;66;03m# Call `pandas_read_text`\u001b[39;00m\n\u001b[1;32m--> 142\u001b[0m df \u001b[38;5;241m=\u001b[39m pandas_read_text(\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreader,\n\u001b[0;32m    144\u001b[0m     block,\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[0;32m    146\u001b[0m     rest_kwargs,\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypes,\n\u001b[0;32m    148\u001b[0m     columns,\n\u001b[0;32m    149\u001b[0m     write_header,\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menforce,\n\u001b[0;32m    151\u001b[0m     path_info,\n\u001b[0;32m    152\u001b[0m )\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m project_after_read:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns]\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:197\u001b[0m, in \u001b[0;36mpandas_read_text\u001b[1;34m(reader, b, header, kwargs, dtypes, columns, write_header, enforce, path)\u001b[0m\n\u001b[0;32m    195\u001b[0m df \u001b[38;5;241m=\u001b[39m reader(bio, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtypes:\n\u001b[1;32m--> 197\u001b[0m     coerce_dtypes(df, dtypes)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m enforce \u001b[38;5;129;01mand\u001b[39;00m columns \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mlist\u001b[39m(df\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlist\u001b[39m(columns)):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumns do not match\u001b[39m\u001b[38;5;124m\"\u001b[39m, df\u001b[38;5;241m.\u001b[39mcolumns, columns)\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:298\u001b[0m, in \u001b[0;36mcoerce_dtypes\u001b[1;34m(df, dtypes)\u001b[0m\n\u001b[0;32m    294\u001b[0m rule \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m61\u001b[39m)\n\u001b[0;32m    295\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMismatched dtypes found in `pd.read_csv`/`pd.read_table`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[0;32m    296\u001b[0m     rule\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, [dtype_msg, date_msg]))\n\u001b[0;32m    297\u001b[0m )\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[1;31mValueError\u001b[0m: Mismatched dtypes found in `pd.read_csv`/`pd.read_table`.\n\n+-------------------+---------+----------+\n| Column            | Found   | Expected |\n+-------------------+---------+----------+\n| ActualElapsedTime | float64 | int64    |\n| ArrDelay          | float64 | int64    |\n| ArrTime           | float64 | int64    |\n| DepDelay          | float64 | int64    |\n| DepTime           | float64 | int64    |\n| Distance          | float64 | int64    |\n+-------------------+---------+----------+\n\nUsually this is due to dask's dtype inference failing, and\n*may* be fixed by specifying dtypes manually by adding:\n\ndtype={'ActualElapsedTime': 'float64',\n       'ArrDelay': 'float64',\n       'ArrTime': 'float64',\n       'DepDelay': 'float64',\n       'DepTime': 'float64',\n       'Distance': 'float64'}\n\nto the call to `read_csv`/`read_table`.\n\nAlternatively, provide `assume_missing=True` to interpret\nall unspecified integer columns as floats."
     ]
    }
   ],
   "source": [
    "ddf = dd.read_csv(files,\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={\"TailNum\": str,\n",
    "                        \"CRSElapsedTime\": float,\n",
    "                        \"Cancelled\": bool})\n",
    "print(ddf.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61959ff",
   "metadata": {},
   "source": [
    "Notice that the representation of the dataframe object contains no data - Dask has just done enough to read the start of the first file, and infer the column names and dtypes.\n",
    "\n",
    "**Dask is lazy!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d714511c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'DayOfWeek', 'DepTime', 'CRSDepTime', 'ArrTime', 'CRSArrTime',\n",
       "       'UniqueCarrier', 'FlightNum', 'TailNum', 'ActualElapsedTime',\n",
       "       'CRSElapsedTime', 'AirTime', 'ArrDelay', 'DepDelay', 'Origin', 'Dest',\n",
       "       'Distance', 'TaxiIn', 'TaxiOut', 'Cancelled', 'Diverted'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a24944c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                  datetime64[ns]\n",
       "DayOfWeek                      int64\n",
       "DepTime                      float64\n",
       "CRSDepTime                     int64\n",
       "ArrTime                      float64\n",
       "CRSArrTime                     int64\n",
       "UniqueCarrier        string[pyarrow]\n",
       "FlightNum                      int64\n",
       "TailNum              string[pyarrow]\n",
       "ActualElapsedTime            float64\n",
       "CRSElapsedTime               float64\n",
       "AirTime                      float64\n",
       "ArrDelay                     float64\n",
       "DepDelay                     float64\n",
       "Origin               string[pyarrow]\n",
       "Dest                 string[pyarrow]\n",
       "Distance                     float64\n",
       "TaxiIn                       float64\n",
       "TaxiOut                      float64\n",
       "Cancelled                       bool\n",
       "Diverted                       int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a86ccb",
   "metadata": {},
   "source": [
    "Dask DataFrames have an `.npartitions` attribute which tells you how many partitions make up a Dask DataFrame.\n",
    "\n",
    "Dask is able to process larger-than-memory datasets by cutting computations into smaller parts and processing those in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b04c8168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.npartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d2fb2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## The pandas Look & Feel\n",
    "Dask DataFrames implement a well-used portion of the Pandas API in a way that allows for parallel and out-of-core computation. This means that a lot of Dask DataFrame code will look and feel familiar to pandas users: \n",
    "\n",
    "```python\n",
    "import pandas as pd                   import dask.dataframe as dd\n",
    "df = pd.read_csv('2015-01-01.csv')    df = dd.read_csv('2015-*-*.csv')\n",
    "df.groupby(df.user_id).value.mean()   df.groupby(df.user_id).value.mean().compute()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87fe45a",
   "metadata": {
    "tags": []
   },
   "source": [
    "This is because, internally, **a Dask DataFrame is composed of many pandas DataFrames**: \n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"50%\">\n",
    "\n",
    "Dask DataFrames are divided into different **partitions** where each partition is a pandas DataFrame. This is why driving the Dask car *can feel* like you're still driving the pandas car: Dask is performing a bunch of regular pandas operations on regular pandas objects under the hood.\n",
    "\n",
    "But don't forget that you've entered the world of distributed computing now -- which means you've added a lot more complexity to the mix. You now need to consider things like concurrency, state, data duplicates, data loss, etc.\n",
    "\n",
    "Luckily, with a high-level Collection like DataFrames, Dask handles most of these complicated questions for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a6d01f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## pandas-like Computations\n",
    "\n",
    "Let's see this in action with a more involved example. Let's compute the largest flight departure delay.\n",
    "\n",
    "In pandas we could do this by iterating over each file to find the individual maximums and then find the final maximum over the individual maximums.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "files = os.listdir(os.path.join('data', 'nycflights'))\n",
    "\n",
    "maxes = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join('data', 'nycflights', file))\n",
    "    maxes.append(df.DepDelay.max())\n",
    "\n",
    "final_max = max(maxes)\n",
    "```\n",
    "\n",
    "Thankfully, we can do this with Dask DataFrames using pandas-like code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a21dc382",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_delay = ddf[\"DepDelay\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3dce768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask_expr.expr.Scalar: expr=ReadCSV(4026eee)['DepDelay'].max(), dtype=float64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_delay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c59405",
   "metadata": {},
   "source": [
    "The above cell looks exactly like what we would do using pandas...but the result does not! \n",
    "\n",
    "Instead of the actual result of the computation, we only get some schematic information. This is because Dask DataFrames are lazily evaluated. This means that **no computation happens unless you explicitly tell Dask to do so** by calling `.compute()`.\n",
    "\n",
    "Before actually performing a computation, dask first constructs a task graph that it can use to optimize computing the result in parallel. You can think of a task graph as the recipe or routemap that contains all the necessary instructions to arrive at the final result. Once you call `.compute()`, Dask will execute the instructions contained in the task graph and perform computations in parallel.\n",
    "\n",
    "Let's look at the task graph to get a feel for how Dask's blocked algorithms work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9c5128d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Pages: 1 -->\n",
       "<svg width=\"313pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 313.00 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-184 309,-184 309,4 -4,4\"/>\n",
       "<!-- 3495382476768370406 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>3495382476768370406</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"231,-180 74,-180 74,-144 231,-144 231,-180\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.5\" y=\"-157\" font-family=\"Helvetica,sans-Serif\" font-size=\"20.00\">Max(Projection)</text>\n",
       "</g>\n",
       "<!-- &#45;6247079956867810573 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>&#45;6247079956867810573</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"305,-108 0,-108 0,-72 305,-72 305,-108\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.5\" y=\"-85\" font-family=\"Helvetica,sans-Serif\" font-size=\"20.00\">Projection(ReadCSV, DepDelay)</text>\n",
       "</g>\n",
       "<!-- &#45;6247079956867810573&#45;&gt;3495382476768370406 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>&#45;6247079956867810573&#45;&gt;3495382476768370406</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.5,-108.3C152.5,-116.02 152.5,-125.29 152.5,-133.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"149,-133.9 152.5,-143.9 156,-133.9 149,-133.9\"/>\n",
       "</g>\n",
       "<!-- &#45;8640647748099632978 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>&#45;8640647748099632978</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"205.5,-36 99.5,-36 99.5,0 205.5,0 205.5,-36\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.5\" y=\"-13\" font-family=\"Helvetica,sans-Serif\" font-size=\"20.00\">ReadCSV</text>\n",
       "</g>\n",
       "<!-- &#45;8640647748099632978&#45;&gt;&#45;6247079956867810573 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>&#45;8640647748099632978&#45;&gt;&#45;6247079956867810573</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M152.5,-36.3C152.5,-44.02 152.5,-53.29 152.5,-61.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"149,-61.9 152.5,-71.9 156,-61.9 149,-61.9\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x262f4f45ca0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_delay.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acae36",
   "metadata": {},
   "source": [
    "Some things to note:\n",
    "\n",
    "1.  Up until this point everything is lazy. To evaluate the result for `max_delay`, call its `compute()` method:\n",
    "2.  Dask will delete intermediate results (like the full pandas DataFrame for each file) as soon as possible.\n",
    "    -  This lets us handle datasets that are larger than memory\n",
    "    -  This means that repeated computations will have to load all of the data in each time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92bb3578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:640: FutureWarning: Support for nested sequences for 'parse_dates' in pd.read_csv is deprecated. Combine the desired columns with pd.to_datetime after parsing instead.\n",
      "  head = reader(BytesIO(b_sample), nrows=sample_rows, **head_kwargs)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\backends.py:140\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:771\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[1;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[0;32m    759\u001b[0m     urlpath,\n\u001b[0;32m    760\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    769\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    770\u001b[0m ):\n\u001b[1;32m--> 771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m read_pandas(\n\u001b[0;32m    772\u001b[0m         reader,\n\u001b[0;32m    773\u001b[0m         urlpath,\n\u001b[0;32m    774\u001b[0m         blocksize\u001b[38;5;241m=\u001b[39mblocksize,\n\u001b[0;32m    775\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m    776\u001b[0m         compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    777\u001b[0m         sample\u001b[38;5;241m=\u001b[39msample,\n\u001b[0;32m    778\u001b[0m         sample_rows\u001b[38;5;241m=\u001b[39msample_rows,\n\u001b[0;32m    779\u001b[0m         enforce\u001b[38;5;241m=\u001b[39menforce,\n\u001b[0;32m    780\u001b[0m         assume_missing\u001b[38;5;241m=\u001b[39massume_missing,\n\u001b[0;32m    781\u001b[0m         storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    782\u001b[0m         include_path_column\u001b[38;5;241m=\u001b[39minclude_path_column,\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    784\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\dataframe\\io\\csv.py:640\u001b[0m, in \u001b[0;36mread_pandas\u001b[1;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[0;32m    639\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 640\u001b[0m     head \u001b[38;5;241m=\u001b[39m reader(BytesIO(b_sample), nrows\u001b[38;5;241m=\u001b[39msample_rows, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhead_kwargs)\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pd\u001b[38;5;241m.\u001b[39merrors\u001b[38;5;241m.\u001b[39mParserError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1898\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1898\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1899\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:161\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_parse_dates_presence(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnames)  \u001b[38;5;66;03m# type: ignore[has-type]\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_noconvert_columns()\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\base_parser.py:248\u001b[0m, in \u001b[0;36mParserBase._validate_parse_dates_presence\u001b[1;34m(self, columns)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# Convert positions to actual column names\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 248\u001b[0m     col \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m columns) \u001b[38;5;28;01melse\u001b[39;00m columns[col]\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m cols_needed\n\u001b[0;32m    250\u001b[0m ]\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed eval>:1\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_collection.py:475\u001b[0m, in \u001b[0;36mFrameBase.compute\u001b[1;34m(self, fuse, **kwargs)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, Scalar):\n\u001b[0;32m    474\u001b[0m     out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mrepartition(npartitions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 475\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse)\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DaskMethodsMixin\u001b[38;5;241m.\u001b[39mcompute(out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_collection.py:590\u001b[0m, in \u001b[0;36mFrameBase.optimize\u001b[1;34m(self, fuse)\u001b[0m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, fuse: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    573\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimizes the DataFrame.\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \n\u001b[0;32m    575\u001b[0m \u001b[38;5;124;03m    Runs the optimizer with all steps over the DataFrame and wraps the result in a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[38;5;124;03m        The optimized Dask Dataframe\u001b[39;00m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 590\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m new_collection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr\u001b[38;5;241m.\u001b[39moptimize(fuse\u001b[38;5;241m=\u001b[39mfuse))\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_expr.py:94\u001b[0m, in \u001b[0;36mExpr.optimize\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m optimize(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_expr.py:3028\u001b[0m, in \u001b[0;36moptimize\u001b[1;34m(expr, fuse)\u001b[0m\n\u001b[0;32m   3007\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"High level query optimization\u001b[39;00m\n\u001b[0;32m   3008\u001b[0m \n\u001b[0;32m   3009\u001b[0m \u001b[38;5;124;03mThis leverages three optimization passes:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[38;5;124;03moptimize_blockwise_fusion\u001b[39;00m\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3026\u001b[0m stage: core\u001b[38;5;241m.\u001b[39mOptimizerStage \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fuse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplified-physical\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 3028\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m optimize_until(expr, stage)\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_expr.py:2979\u001b[0m, in \u001b[0;36moptimize_until\u001b[1;34m(expr, stage)\u001b[0m\n\u001b[0;32m   2976\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m   2978\u001b[0m \u001b[38;5;66;03m# Simplify\u001b[39;00m\n\u001b[1;32m-> 2979\u001b[0m expr \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39msimplify()\n\u001b[0;32m   2980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimplified-logical\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m expr\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_core.py:371\u001b[0m, in \u001b[0;36mExpr.simplify\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    370\u001b[0m     dependents \u001b[38;5;241m=\u001b[39m collect_dependents(expr)\n\u001b[1;32m--> 371\u001b[0m     new \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39msimplify_once(dependents\u001b[38;5;241m=\u001b[39mdependents, simplified\u001b[38;5;241m=\u001b[39m{})\n\u001b[0;32m    372\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m==\u001b[39m expr\u001b[38;5;241m.\u001b[39m_name:\n\u001b[0;32m    373\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_core.py:349\u001b[0m, in \u001b[0;36mExpr.simplify_once\u001b[1;34m(self, dependents, simplified)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(operand, Expr):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;66;03m# Bandaid for now, waiting for Singleton\u001b[39;00m\n\u001b[0;32m    348\u001b[0m     dependents[operand\u001b[38;5;241m.\u001b[39m_name]\u001b[38;5;241m.\u001b[39mappend(weakref\u001b[38;5;241m.\u001b[39mref(expr))\n\u001b[1;32m--> 349\u001b[0m     new \u001b[38;5;241m=\u001b[39m operand\u001b[38;5;241m.\u001b[39msimplify_once(\n\u001b[0;32m    350\u001b[0m         dependents\u001b[38;5;241m=\u001b[39mdependents, simplified\u001b[38;5;241m=\u001b[39msimplified\n\u001b[0;32m    351\u001b[0m     )\n\u001b[0;32m    352\u001b[0m     simplified[operand\u001b[38;5;241m.\u001b[39m_name] \u001b[38;5;241m=\u001b[39m new\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m!=\u001b[39m operand\u001b[38;5;241m.\u001b[39m_name:\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_core.py:322\u001b[0m, in \u001b[0;36mExpr.simplify_once\u001b[1;34m(self, dependents, simplified)\u001b[0m\n\u001b[0;32m    319\u001b[0m expr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 322\u001b[0m     out \u001b[38;5;241m=\u001b[39m expr\u001b[38;5;241m.\u001b[39m_simplify_down()\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m         out \u001b[38;5;241m=\u001b[39m expr\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_expr.py:2072\u001b[0m, in \u001b[0;36mProjection._simplify_down\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_simplify_down\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   2070\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2071\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m-> 2072\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39m_meta\u001b[38;5;241m.\u001b[39mndim\n\u001b[0;32m   2073\u001b[0m     ):\n\u001b[0;32m   2074\u001b[0m         \u001b[38;5;66;03m# TODO: we should get more precise around Expr.columns types\u001b[39;00m\n\u001b[0;32m   2075\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\n\u001b[0;32m   2076\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe, Projection):\n\u001b[0;32m   2077\u001b[0m         \u001b[38;5;66;03m# df[a][b]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\functools.py:995\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    993\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 995\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    997\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\_expr.py:2045\u001b[0m, in \u001b[0;36mProjection._meta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2043\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[0;32m   2044\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m-> 2045\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataframe_like(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;241m.\u001b[39m_meta):\n\u001b[0;32m   2046\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_meta\n\u001b[0;32m   2047\u001b[0m     \u001b[38;5;66;03m# if we are not a DataFrame and have a scalar, we reduce to a scalar\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\functools.py:995\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    993\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 995\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    997\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\io\\csv.py:85\u001b[0m, in \u001b[0;36mReadCSV._meta\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mcached_property\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ddf\u001b[38;5;241m.\u001b[39m_meta\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\functools.py:995\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    993\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 995\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(instance)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    997\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask_expr\\io\\csv.py:75\u001b[0m, in \u001b[0;36mReadCSV._ddf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m         meta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation(\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename,\n\u001b[0;32m     69\u001b[0m             header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[0;32m     70\u001b[0m             storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m     71\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     72\u001b[0m         )\u001b[38;5;241m.\u001b[39m_meta\n\u001b[0;32m     73\u001b[0m         columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(meta\u001b[38;5;241m.\u001b[39mcolumns)[\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moperation(\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename,\n\u001b[0;32m     77\u001b[0m     usecols\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m     78\u001b[0m     header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheader,\n\u001b[0;32m     79\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m     81\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\troy5\\anaconda3\\Lib\\site-packages\\dask\\backends.py:142\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    146\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: list index out of range"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "max_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05540de7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## More Dask DataFrame computations\n",
    "\n",
    "Let's see couple of examples on how the API for Dask DataFrames is the same than Pandas. If you are comfortable with Pandas, the following operations will look very familiar, except we will need to add the `compute()` to get the results wanted.\n",
    "\n",
    "### Example 1: Total of non-cancelled flights taken\n",
    "\n",
    "Notice that there is a column in our DataFrame called `\"Cancelled\"` that is a boolean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0883849a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(~ddf[\"Cancelled\"]).sum().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df23079",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example 2: Total of non-cancelled flights taken by airport\n",
    "\n",
    "We should select the non-canceled flights, use the operation `groupby` on the `\"Origin\"` column and finally use `count` to get the detailed per airport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b01ecd4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf[~ddf[\"Cancelled\"]].groupby(\"Origin\")[\"Origin\"].count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dec6b4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 1: What is the average departure delay from each airport?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5668216",
   "metadata": {},
   "source": [
    "Uncomment and run the cell below to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6319550d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf.groupby(\"Origin\")[\"DepDelay\"].mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230c37b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Exercise 2: What day of the week has the worst average departure delay?\n",
    "Uncomment and run the cell below to see the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dacd560",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ddf.groupby(\"DayOfWeek\")[\"DepDelay\"].mean().idxmax().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da1c43",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Working with Partitions\n",
    "Dask DataFrames implements a large portion of the pandas API by simply performing pandas methods on its partitions (which are pandas objects). \n",
    "\n",
    "### Mapping Functions with `map_partitions`\n",
    "However, sometimes you might want to manipulate your Dask DataFrame with a custom function. You can use the `map_partitions` method for this.\n",
    "\n",
    "Imagine you find out that there was a 2-minute error in the `DepDelay` column.\n",
    "\n",
    "Let's create a pandas `apply` function that will subtract 2 from every input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca97e0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subtract_2(df):\n",
    "    return df.apply(lambda x: x-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5ffd7a",
   "metadata": {},
   "source": [
    "We can then map this function over all of our partitions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db73a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"Adjusted_DepDelay\"] = ddf[\"DepDelay\"].map_partitions(subtract_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f79e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[[\"DepDelay\", \"Adjusted_DepDelay\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45884f8b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance tip: When to call .compute()\n",
    "\n",
    "In the examples and exercises above, we sometimes perform the same operation more than once (e.g. `read_csv`). Dask DataFrames hashes the arguments, allowing duplicate computations to be shared, and only computed once. You can use `dask.compute()` to merge task graphs of multiple operations.\n",
    "\n",
    "For example, let's compute the mean and standard deviation for departure delay of all non-canceled flights. Since Dask operations are lazy, those values aren't the final results until we `compute` them. They're just the recipe required to get the result.\n",
    "\n",
    "If we compute them with two calls to compute, there is no sharing of intermediate computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540952b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = ddf[~ddf[\"Cancelled\"]]\n",
    "mean_delay = non_cancelled[\"DepDelay\"].mean()\n",
    "std_delay = non_cancelled[\"DepDelay\"].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da852348",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_result = mean_delay.compute()\n",
    "std_delay_result = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6c61e",
   "metadata": {},
   "source": [
    "Now, let's see how long it takes if we try computing `mean_delay` and `std_delay` with a single `compute()` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1514304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefc4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db683a",
   "metadata": {},
   "source": [
    "Using `dask.compute` takes roughly 1/2 the time. This is because the task graphs for both results are merged when calling `dask.compute`, allowing shared operations (like `read_csv`) to only be done once instead of twice. In particular, using `dask.compute` only does the following once:\n",
    "\n",
    "- The calls to `read_csv`\n",
    "- The filter (`df[~df[\"Cancelled\"]]`)\n",
    "- The `\"DepDelay\"` column indexing\n",
    "- Some of the necessary reductions (`sum`, `count`)\n",
    "\n",
    "To see what the merged task graphs between multiple results look like (and what's shared), you can use the `dask.visualize` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc371ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b983d2a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Extra resources\n",
    "\n",
    "- Explore applying custom code to Dask DataFrames: [Dask Tutorial DataFrames](https://github.com/dask/dask-tutorial/blob/main/04_dataframe.ipynb)\n",
    "- [Dask DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "- [Dask DataFrame examples](https://examples.dask.org/dataframe.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
